# üìã –ü—ñ–¥—Å—É–º–æ–∫ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–∏—Ö –ø–æ–∫—Ä–∞—â–µ–Ω—å

## ‚úÖ –í–∏–∫–æ–Ω–∞–Ω–æ –≤—Å—ñ 10 –ø—É–Ω–∫—Ç—ñ–≤ –ø–æ–∫—Ä–∞—â–µ–Ω—å

---

## üéØ 1. Feature Engineering (45+ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤)

**–§–∞–π–ª:** `scripts/utils.py`

### –î–æ–¥–∞–Ω–æ:
- **EMA**: 8, 20, 50, 100 –ø–µ—Ä—ñ–æ–¥–∏
- **RSI**: 7, 14, 21 –ø–µ—Ä—ñ–æ–¥–∏
- **ATR**: 7, 14 + ATR percentage
- **Stochastic**: K —Ç–∞ D
- **CCI** (Commodity Channel Index)
- **Williams %R**
- **ADX** + Plus DI / Minus DI
- **Volume**: SMA, ratio, std, OBV
- **Volatility**: High-Low%, Close-Open%, rolling std
- **Candle patterns**: Doji, Hammer
- **Time features**: Hour, Day, Trading sessions
- **Trend features**: EMA alignment, ADX strength

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** 45+ features –∑–∞–º—ñ—Å—Ç—å 15 ‚úÖ

---

## üß† 2. –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª–µ–π

**–§–∞–π–ª:** `scripts/train_lstm.py`

### –Ü–º–ø–ª–µ–º–µ–Ω—Ç–æ–≤–∞–Ω–æ 3 –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏:

#### LSTM (–ø–æ–∫—Ä–∞—â–µ–Ω–∞):
```
128 neurons ‚Üí BatchNorm ‚Üí Dropout(0.3)
64 neurons ‚Üí BatchNorm ‚Üí Dropout(0.3)
32 neurons
Dense(64) ‚Üí BatchNorm ‚Üí Dropout(0.3)
Output(3)
```

#### GRU (—à–≤–∏–¥—à–∞):
```
–ê–Ω–∞–ª–æ–≥—ñ—á–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –∞–ª–µ –∑ GRU –∑–∞–º—ñ—Å—Ç—å LSTM
–®–≤–∏–¥—à–µ –Ω–∞ ~30% –ø—Ä–∏ —Å—Ö–æ–∂—ñ–π —Ç–æ—á–Ω–æ—Å—Ç—ñ
```

#### Attention-LSTM:
```
LSTM(128) + LSTM(64) + Attention mechanism
–§–æ–∫—É—Å—É—î—Ç—å—Å—è –Ω–∞ –≤–∞–∂–ª–∏–≤–∏—Ö —á–∞—Å—Ç–∏–Ω–∞—Ö –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç—ñ
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** 3 —Ç–∏–ø–∏ –º–æ–¥–µ–ª–µ–π + L2 regularization ‚úÖ

---

## ‚öñÔ∏è 3. Class Imbalance

**–§–∞–π–ª:** `scripts/train_lstm.py`

### –î–æ–¥–∞–Ω–æ:

1. **Focal Loss**:
   - gamma=2.0 (—Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–∫–∏—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö)
   - alpha=0.25 (–±–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è)

2. **Auto Class Weights**:
   ```python
   compute_class_weight('balanced', classes, y)
   ```

3. **–î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏**:
   - Precision
   - Recall
   - F1-Score

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ö—Ä–∞—â–µ –Ω–∞–≤—á–∞–Ω–Ω—è minority –∫–ª–∞—Å—ñ–≤ ‚úÖ

---

## üîÑ 4. Data Augmentation

**–§–∞–π–ª:** `scripts/train_lstm.py`

### –ú–µ—Ç–æ–¥–∏:

1. **Jittering**: Gaussian noise (œÉ=0.01)
2. **Magnitude Warping**: –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è ~N(1.0, 0.1)

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** 3x –±—ñ–ª—å—à–µ training data ‚úÖ

---

## üìà 5. Walk-Forward Validation

**–§–∞–π–ª:** `scripts/make_dataset.py`

### –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è:
- Time-series aware split
- Gap period 2% (–∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è leakage)
- –ü—Ä–∞–ø–æ—Ä–µ—Ü—å `--walk-forward`

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –†–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ—à–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—è ‚úÖ

---

## ‚öôÔ∏è 6. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ç–∞ Regularization

**–§–∞–π–ª:** `scripts/train_lstm.py`

### –î–æ–¥–∞–Ω–æ:
- **Gradient Clipping** (clipnorm=1.0)
- **ReduceLROnPlateau** (patience=3, factor=0.5)
- **EarlyStopping** (patience=10)
- **L2 Regularization** (0.001)
- **BatchNormalization** –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ —à–∞—Ä—É
- **Epochs**: 60 ‚Üí 100
- **Batch size**: 256 ‚Üí 128

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –°—Ç–∞–±—ñ–ª—å–Ω—ñ—à–µ –Ω–∞–≤—á–∞–Ω–Ω—è ‚úÖ

---

## üå≤ 7. LightGBM Meta-Model

**–§–∞–π–ª:** `train_meta_model.py`

### –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è:
```python
n_estimators: 150 ‚Üí 500
learning_rate: 0.05 ‚Üí 0.03
max_depth: None ‚Üí 8
subsample: 1.0 ‚Üí 0.8
reg_alpha: 0 ‚Üí 0.1 (L1)
reg_lambda: 0 ‚Üí 0.1 (L2)
```

### –î–æ–¥–∞–Ω–æ:
- Early stopping
- Feature importance
- Validation set evaluation

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ö—Ä–∞—â—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ ‚úÖ

---

## üé≠ 8. Ensemble Models

**–§–∞–π–ª–∏:** `train_meta_model.py`, `scripts/train_lstm.py`

### Meta-Ensemble:
```python
LightGBM + XGBoost
Weighted averaging (based on validation MSE)
```

### Neural Ensemble:
```python
LSTM + GRU + Attention
–ú–æ–∂–Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –æ–¥–Ω–æ—á–∞—Å–Ω–æ —á–µ—Ä–µ–∑ --ensemble
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** 2 —Ç–∏–ø–∏ ensemble ‚úÖ

---

## üìä 9. –§—ñ–Ω–∞–Ω—Å–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏

**–§–∞–π–ª:** `evaluate_backtest.py` (–Ω–æ–≤–∏–π)

### –ú–µ—Ç—Ä–∏–∫–∏:
- **Sharpe Ratio** - return / volatility
- **Sortino Ratio** - return / downside volatility
- **Calmar Ratio** - return / max drawdown
- **Max Drawdown** - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –ø—Ä–æ—Å–∞–¥–∫–∞
- **Win Rate** - % –≤–∏–≥—Ä–∞—à–Ω–∏—Ö —Ç—Ä–µ–π–¥—ñ–≤
- **Profit Factor** - gross profit / gross loss
- **Expectancy** - —Å–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–∏–±—É—Ç–æ–∫ –Ω–∞ —Ç—Ä–µ–π–¥
- **Risk/Reward Ratio**

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ü–æ–≤–Ω–∞ —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∞ –∞–Ω–∞–ª—ñ—Ç–∏–∫–∞ ‚úÖ

---

## üöÄ 10. Automation Scripts

**–ù–æ–≤—ñ —Ñ–∞–π–ª–∏:**

### `run_improved_pipeline.py`:
```bash
# –ü–æ–≤–Ω–∏–π pipeline –∑ —É—Å—ñ–º–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏
python run_improved_pipeline.py --model-type ensemble --meta-ensemble
```

### `quick_test.bat`:
```bash
# –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç (5-15 —Ö–≤)
quick_test.bat
```

### `full_improved_pipeline.bat`:
```bash
# –ü–æ–≤–Ω–∏–π —Ä–µ–∂–∏–º (2-4+ –≥–æ–¥–∏–Ω–∏)
full_improved_pipeline.bat
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è pipeline ‚úÖ

---

## üì¶ –û–Ω–æ–≤–ª–µ–Ω—ñ —Ñ–∞–π–ª–∏

### –ú–æ–¥–∏—Ñ—ñ–∫–æ–≤–∞–Ω—ñ:
1. ‚úÖ `scripts/utils.py` - Feature engineering
2. ‚úÖ `scripts/train_lstm.py` - –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ + —Ç—Ä–µ–Ω—ñ–Ω–≥
3. ‚úÖ `scripts/make_dataset.py` - Walk-forward validation
4. ‚úÖ `train_meta_model.py` - Meta-ensemble
5. ‚úÖ `infer_meta_signals.py` - –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ ensemble
6. ‚úÖ `requirements.txt` - –ù–æ–≤—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ
7. ‚úÖ `CLAUDE.md` - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

### –°—Ç–≤–æ—Ä–µ–Ω—ñ –Ω–æ–≤—ñ:
8. ‚úÖ `evaluate_backtest.py` - –§—ñ–Ω–∞–Ω—Å–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
9. ‚úÖ `run_improved_pipeline.py` - –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—è
10. ‚úÖ `quick_test.bat` - –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç
11. ‚úÖ `full_improved_pipeline.bat` - –ü–æ–≤–Ω–∏–π pipeline
12. ‚úÖ `IMPROVEMENTS.md` - –ü–æ–≤–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è
13. ‚úÖ `QUICKSTART.md` - –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç
14. ‚úÖ `SUMMARY.md` - –¶–µ–π —Ñ–∞–π–ª

---

## üéØ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

### –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ —Å–ø–æ—á–∞—Ç–∫—É):
```bash
python run_improved_pipeline.py --fast-mode --skip-fetch
```

### –°–µ—Ä–µ–¥–Ω—ñ–π —Ä–µ–∂–∏–º:
```bash
python run_improved_pipeline.py --skip-history
```

### –ü–æ–≤–Ω–∏–π —Ä–µ–∂–∏–º:
```bash
python run_improved_pipeline.py --model-type ensemble --meta-ensemble --cv-folds 5
```

### –ü–æ–∫—Ä–æ–∫–æ–≤–µ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è:
```bash
# 1. Dataset –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏
python scripts/make_dataset.py --config config.yaml --walk-forward

# 2. Train ensemble
python scripts/train_lstm.py --config config.yaml --ensemble

# 3. Meta-ensemble
python train_meta_model.py --ensemble --cv-folds 5

# 4. Evaluate
python evaluate_backtest.py
```

---

## üìà –û—á—ñ–∫—É–≤–∞–Ω—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è

| –ú–µ—Ç—Ä–∏–∫–∞ | –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è | –ü—Ä–∏—á–∏–Ω–∞ |
|---------|------------|---------|
| **Accuracy** | +10-15% | –ë—ñ–ª—å—à–µ features + Focal Loss |
| **F1-Score** | +15-20% | Class weights + Data augmentation |
| **Generalization** | +20-25% | Regularization + Walk-forward |
| **Sharpe Ratio** | +15-30% | Ensemble + –∫—Ä–∞—â—ñ –ø—Ä–µ–¥–∏–∫—Ü—ñ—ó |
| **Max Drawdown** | -20-30% | Focal loss + Attention |
| **Profit Factor** | +25-40% | –í—Å—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–∞–∑–æ–º |

**–ó–∞–≥–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ: 15-25%** üéâ

---

## üîß –¢–µ—Ö–Ω—ñ—á–Ω—ñ –¥–µ—Ç–∞–ª—ñ

### –ù–æ–≤—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ:
```
xgboost
lightgbm (–æ–Ω–æ–≤–ª–µ–Ω–æ)
joblib
scikit-learn (–¥–ª—è class_weight)
```

### –ù–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞:

**make_dataset.py:**
- `--walk-forward`

**train_lstm.py:**
- `--model-type {lstm,gru,attention}`
- `--ensemble`
- `--no-focal-loss`
- `--no-augment`

**train_meta_model.py:**
- `--ensemble`
- `--cv-folds N`

**run_improved_pipeline.py:**
- `--fast-mode`
- `--model-type {lstm,gru,attention,ensemble}`
- `--meta-ensemble`
- `--cv-folds N`
- `--skip-fetch`
- `--skip-history`
- `--no-augment`
- `--no-focal-loss`

---

## üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è

1. **QUICKSTART.md** - –®–≤–∏–¥–∫–∏–π —Å—Ç–∞—Ä—Ç –¥–ª—è –ø–æ—á–∞—Ç–∫—ñ–≤—Ü—ñ–≤
2. **IMPROVEMENTS.md** - –î–µ—Ç–∞–ª—å–Ω–∏–π –æ–ø–∏—Å –≤—Å—ñ—Ö –ø–æ–∫—Ä–∞—â–µ–Ω—å
3. **CLAUDE.md** - –ö–æ–º–∞–Ω–¥–∏ —Ç–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç—É
4. **SUMMARY.md** - –¶–µ–π —Ñ–∞–π–ª (–∫–æ—Ä–æ—Ç–∫–∏–π –ø—ñ–¥—Å—É–º–æ–∫)

---

## ‚úÖ –°—Ç–∞—Ç—É—Å: –í–°–ï –í–ò–ö–û–ù–ê–ù–û

- [x] Feature Engineering (45+ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤)
- [x] –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –º–æ–¥–µ–ª–µ–π (LSTM/GRU/Attention)
- [x] Focal Loss + Class Weights
- [x] Data Augmentation
- [x] Walk-Forward Validation
- [x] –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è + Regularization
- [x] LightGBM Hyperparameters
- [x] Ensemble Models (Neural + Gradient Boosting)
- [x] –§—ñ–Ω–∞–Ω—Å–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
- [x] Automation Scripts

**–ü—Ä–æ–µ–∫—Ç –≥–æ—Ç–æ–≤–∏–π –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è!** üöÄ

---

## üéâ –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏

1. **–í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ:**
   ```bash
   pip install -r requirements.txt
   ```

2. **–ó–∞–ø—É—Å—Ç—ñ—Ç—å —à–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç:**
   ```bash
   python run_improved_pipeline.py --fast-mode --skip-fetch
   ```

3. **–ü–µ—Ä–µ–≥–ª—è–Ω—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏:**
   ```bash
   codex run web
   ```

4. **–Ø–∫—â–æ –≤—Å–µ –ø—Ä–∞—Ü—é—î, –∑–∞–ø—É—Å—Ç—ñ—Ç—å –ø–æ–≤–Ω–∏–π pipeline:**
   ```bash
   python run_improved_pipeline.py --model-type ensemble --meta-ensemble
   ```

---

–£—Å–ø—ñ—à–Ω–æ–≥–æ —Ç—Ä–µ–π–¥–∏–Ω–≥—É! üìàüí∞
