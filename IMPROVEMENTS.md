# üöÄ –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏ Forex LSTM

## –û–≥–ª—è–¥ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–∏—Ö –ø–æ–∫—Ä–∞—â–µ–Ω—å

–¶–µ–π –¥–æ–∫—É–º–µ–Ω—Ç –æ–ø–∏—Å—É—î –≤—Å—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è, —è–∫—ñ –±—É–ª–∏ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω—ñ –¥–ª—è –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π.

---

## üìä 1. –†–æ–∑—à–∏—Ä–µ–Ω–∏–π Feature Engineering

### –î–æ–¥–∞–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏:

**–¢–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏:**
- Multiple EMA (8, 20, 50, 100)
- Multiple RSI (7, 14, 21)
- Multiple ATR (7, 14) + ATR percentage
- Stochastic Oscillator (K, D)
- CCI (Commodity Channel Index)
- Williams %R
- ADX (Average Directional Index) + Plus DI / Minus DI

**Volume features:**
- Volume SMA and ratio
- On-Balance Volume (OBV)
- Volume standard deviation

**Volatility features:**
- High-Low percentage
- Close-Open percentage
- Rolling volatility (20, 50 periods)

**Price position features:**
- Close relative to High/Low
- Distance from moving averages

**Trend confirmation:**
- Multiple EMA alignment
- Strong trend detection (ADX > 25)

**Candle patterns:**
- Doji detection
- Hammer pattern

**Time-based features:**
- Hour of day
- Day of week
- Trading sessions (Asian, London, NY)
- Special days (Monday, Friday)

**–£—Å—å–æ–≥–æ –¥–æ–¥–∞–Ω–æ:** ~45 –Ω–æ–≤–∏—Ö features –∑–∞–º—ñ—Å—Ç—å –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ—Ö 15

---

## üß† 2. –ü–æ–∫—Ä–∞—â–µ–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–µ–π

### LSTM –º–æ–¥–µ–ª—å (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º):
```python
- LSTM(128) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- LSTM(64) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- LSTM(32)
- Dense(64) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- Dense(3, softmax)
```

### GRU –º–æ–¥–µ–ª—å (—à–≤–∏–¥—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞):
```python
- GRU(128) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- GRU(64) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- GRU(32)
- Dense(64) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- Dense(3, softmax)
```

### LSTM –∑ Attention:
```python
- LSTM(128) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- LSTM(64) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- Attention mechanism
- Dense(64) ‚Üí BatchNorm ‚Üí Dropout(0.3)
- Dense(3, softmax)
```

**–ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è:**
- ‚úÖ –ó–±—ñ–ª—å—à–µ–Ω–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ–π—Ä–æ–Ω—ñ–≤ (64‚Üí32 ‚Üí 128‚Üí64‚Üí32)
- ‚úÖ –î–æ–¥–∞–Ω–æ BatchNormalization –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ
- ‚úÖ –ó–±—ñ–ª—å—à–µ–Ω–æ Dropout (0.2 ‚Üí 0.3)
- ‚úÖ –î–æ–¥–∞–Ω–æ L2 regularization (0.001)

---

## ‚öñÔ∏è 3. Class Imbalance —Ç–∞ Loss Function

### Focal Loss:
–ó–∞–º—ñ—Å—Ç—å –ø—Ä–æ—Å—Ç–æ–≥–æ categorical crossentropy –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è Focal Loss –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:
- `gamma=2.0` - —Ñ–æ–∫—É—Å –Ω–∞ –≤–∞–∂–∫–∏—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö
- `alpha=0.25` - –±–∞–ª–∞–Ω—Å—É–≤–∞–Ω–Ω—è –∫–ª–∞—Å—ñ–≤

### Class Weights:
–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤–∞–≥ –∫–ª–∞—Å—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –¥–∞–Ω–∏—Ö:
```python
compute_class_weight('balanced', classes=unique_classes, y=y_train)
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ö—Ä–∞—â–µ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ minority –∫–ª–∞—Å–∞—Ö (LONG/SHORT –ø—Ä–æ—Ç–∏ NO signal)

---

## üîÑ 4. Data Augmentation

–î–æ–¥–∞–Ω–æ –¥–≤–∞ –º–µ—Ç–æ–¥–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó –¥–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤:

1. **Jittering** - –¥–æ–¥–∞–≤–∞–Ω–Ω—è Gaussian noise (œÉ=0.01)
2. **Magnitude Warping** - –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –∑ –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–æ–º ~N(1.0, 0.1)

**–ü—Ä–∏—Ä—ñ—Å—Ç –¥–∞–Ω–∏—Ö:** –∫–æ–∂–µ–Ω —Å–µ–º–ø–ª –≥–µ–Ω–µ—Ä—É—î 2 –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö ‚Üí 3x –±—ñ–ª—å—à–µ training data

---

## üìà 5. Walk-Forward Validation

–ó–∞–º—ñ—Å—Ç—å –ø—Ä–æ—Å—Ç–æ–≥–æ split 70/15/15 –¥–æ–¥–∞–Ω–æ:
- Time-series aware split
- Gap period (2%) –º—ñ–∂ train —ñ validation –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è data leakage
- –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —á–µ—Ä–µ–∑ —Ñ–ª–∞–≥ `--walk-forward`

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**
```bash
python scripts/make_dataset.py --config config.yaml --walk-forward
```

---

## üéØ 6. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è —Ç–∞ Regularization

### Optimizer:
- Gradient clipping (clipnorm=1.0)
- Adam optimizer –∑ adaptive learning rate

### Callbacks:
- ReduceLROnPlateau (patience=3, factor=0.5, min_lr=1e-6)
- EarlyStopping (patience=10, restore_best_weights=True)

### Epochs:
- –ó–±—ñ–ª—å—à–µ–Ω–æ –∑ 60 –¥–æ 100 –∑ early stopping

### Batch size:
- –ó–º–µ–Ω—à–µ–Ω–æ –∑ 256 –¥–æ 128 –¥–ª—è –∫—Ä–∞—â–æ—ó –≥–µ–Ω–µ—Ä–∞–ª—ñ–∑–∞—Ü—ñ—ó

---

## üå≤ 7. –ü–æ–∫—Ä–∞—â–µ–Ω–∏–π LightGBM Meta-Model

### –ù–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:
```python
n_estimators: 500 (–±—É–ª–æ 150)
learning_rate: 0.03 (–±—É–ª–æ 0.05)
max_depth: 8
min_child_samples: 20
subsample: 0.8
colsample_bytree: 0.8
reg_alpha: 0.1 (L1)
reg_lambda: 0.1 (L2)
```

### Early Stopping:
- –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è validation set
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –∑—É–ø–∏–Ω–∫–∞ —á–µ—Ä–µ–∑ 50 —Ä–∞—É–Ω–¥—ñ–≤ –±–µ–∑ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è

### Feature Importance:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ features
- –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ç–æ–ø-10 –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö features

---

## üé≠ 8. Ensemble Models

### LightGBM + XGBoost Ensemble:
```bash
python train_meta_model.py --ensemble
```

**Weighted averaging** –Ω–∞ –æ—Å–Ω–æ–≤—ñ validation performance:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤–∞–≥
- –ö–æ–º–±—ñ–Ω–∞—Ü—ñ—è –ø—Ä–µ–¥–∏–∫—Ü—ñ–π –¥–≤–æ—Ö –º–æ–¥–µ–ª–µ–π

### Multi-Architecture Ensemble (LSTM + GRU + Attention):
```bash
python scripts/train_lstm.py --config config.yaml --ensemble
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** 3 —Ä—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ symbol/timeframe

---

## üìä 9. –§—ñ–Ω–∞–Ω—Å–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏

–ù–æ–≤–∏–π —Å–∫—Ä–∏–ø—Ç `evaluate_backtest.py` —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—î:

### Risk-Adjusted Metrics:
- **Sharpe Ratio** - return / volatility
- **Sortino Ratio** - return / downside volatility
- **Calmar Ratio** - return / max drawdown

### Performance Metrics:
- **Max Drawdown** - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –ø—Ä–æ—Å–∞–¥–∫–∞
- **Win Rate** - –≤—ñ–¥—Å–æ—Ç–æ–∫ –≤–∏–≥—Ä–∞—à–Ω–∏—Ö —Ç—Ä–µ–π–¥—ñ–≤
- **Profit Factor** - gross profit / gross loss
- **Expectancy** - –º–∞—Ç–µ–º–∞—Ç–∏—á–Ω–µ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –Ω–∞ —Ç—Ä–µ–π–¥
- **Risk/Reward Ratio** - —Å–µ—Ä–µ–¥–Ω—î —Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è

**–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:**
```bash
python evaluate_backtest.py --history-dir outputs/history
```

---

## üìù 10. –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ

### –ú–µ—Ç—Ä–∏–∫–∏ –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è:
- Accuracy
- Precision
- Recall
- F1-Score (calculated)

### –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä—ñ—ó:
- Training history –∑–±–µ—Ä—ñ–≥–∞—î—Ç—å—Å—è —É JSON
- –ú–æ–∂–ª–∏–≤—ñ—Å—Ç—å –∞–Ω–∞–ª—ñ–∑—É learning curves

### Time-Series Cross-Validation:
```bash
python train_meta_model.py --cv-folds 5
```

---

## üöÄ –Ø–∫ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ

### 1. –ë–∞–∑–æ–≤–µ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ —É—Å—ñ–º–∞ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è–º–∏:
```bash
# –°—Ç–≤–æ—Ä–∏—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑ walk-forward validation
python scripts/make_dataset.py --config config.yaml --walk-forward

# –¢—Ä–µ–Ω—É–≤–∞—Ç–∏ LSTM –∑ focal loss —Ç–∞ augmentation
python scripts/train_lstm.py --config config.yaml

# –ê–±–æ —Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –≤—Å—ñ —Ç—Ä–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏
python scripts/train_lstm.py --config config.yaml --ensemble
```

### 2. –í–∏–±—ñ—Ä –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—ó –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏:
```bash
# LSTM (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
python scripts/train_lstm.py --config config.yaml --model-type lstm

# GRU (—à–≤–∏–¥—à–∏–π)
python scripts/train_lstm.py --config config.yaml --model-type gru

# LSTM –∑ Attention
python scripts/train_lstm.py --config config.yaml --model-type attention
```

### 3. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è meta-–º–æ–¥–µ–ª—ñ:
```bash
# LightGBM (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º)
python train_meta_model.py --dataset meta_dataset.csv

# Ensemble LightGBM + XGBoost
python train_meta_model.py --dataset meta_dataset.csv --ensemble

# –ó cross-validation
python train_meta_model.py --dataset meta_dataset.csv --cv-folds 5
```

### 4. –û—Ü—ñ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤:
```bash
# Backtest –∑ —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
python evaluate_backtest.py --history-dir outputs/history --initial-balance 10000
```

---

## üìà –û—á—ñ–∫—É–≤–∞–Ω—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è

–ù–∞ –æ—Å–Ω–æ–≤—ñ –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω–∏—Ö –∑–º—ñ–Ω, –æ—á—ñ–∫—É—î—Ç—å—Å—è:

1. **–ü—ñ–¥–≤–∏—â–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—Å—Ç—ñ** –Ω–∞ 10-20% –∑–∞–≤–¥—è–∫–∏:
   - –†–æ–∑—à–∏—Ä–µ–Ω–æ–º—É feature engineering
   - Focal loss —Ç–∞ class weights
   - Data augmentation

2. **–ö—Ä–∞—â–∞ –≥–µ–Ω–µ—Ä–∞–ª—ñ–∑–∞—Ü—ñ—è** –∑–∞–≤–¥—è–∫–∏:
   - BatchNormalization
   - Dropout 0.3
   - L2 regularization
   - Walk-forward validation

3. **–°—Ç–∞–±—ñ–ª—å–Ω—ñ—à—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏** –∑–∞–≤–¥—è–∫–∏:
   - Gradient clipping
   - Early stopping
   - Ensemble –º–æ–¥–µ–ª–µ–π

4. **–ö—Ä–∞—â—ñ —Ñ—ñ–Ω–∞–Ω—Å–æ–≤—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏:**
   - –í–∏—â–∏–π Sharpe Ratio
   - –ú–µ–Ω—à–∏–π Max Drawdown
   - –í–∏—â–∏–π Profit Factor

---

## üîß –¢–µ—Ö–Ω—ñ—á–Ω—ñ –¥–µ—Ç–∞–ª—ñ

### –ù–æ–≤—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ:
- `xgboost` - –¥–ª—è ensemble
- `joblib` - –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
- –û–Ω–æ–≤–ª–µ–Ω–æ `lightgbm` –∑ –∫—Ä–∞—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

### –û–Ω–æ–≤–ª–µ–Ω—ñ —Ñ–∞–π–ª–∏:
- ‚úÖ `scripts/utils.py` - —Ä–æ–∑—à–∏—Ä–µ–Ω–∏–π feature engineering
- ‚úÖ `scripts/train_lstm.py` - –Ω–æ–≤—ñ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ —Ç–∞ —Ç—Ä–µ–Ω—ñ–Ω–≥
- ‚úÖ `scripts/make_dataset.py` - walk-forward validation
- ‚úÖ `train_meta_model.py` - –ø–æ–∫—Ä–∞—â–µ–Ω—ñ hyperparameters + ensemble
- ‚úÖ `infer_meta_signals.py` - –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ ensemble –º–æ–¥–µ–ª–µ–π
- ‚úÖ `evaluate_backtest.py` - –Ω–æ–≤–∏–π —Å–∫—Ä–∏–ø—Ç —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö –º–µ—Ç—Ä–∏–∫
- ‚úÖ `requirements.txt` - –æ–Ω–æ–≤–ª–µ–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ

### –ù–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞:

**make_dataset.py:**
- `--walk-forward` - –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ walk-forward validation

**train_lstm.py:**
- `--model-type {lstm,gru,attention}` - —Ç–∏–ø –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏
- `--no-focal-loss` - –≤–∏–º–∫–Ω—É—Ç–∏ focal loss
- `--no-augment` - –≤–∏–º–∫–Ω—É—Ç–∏ augmentation
- `--ensemble` - —Ç—Ä–µ–Ω—É–≤–∞—Ç–∏ –≤—Å—ñ —Ç—Ä–∏ —Ç–∏–ø–∏ –º–æ–¥–µ–ª–µ–π

**train_meta_model.py:**
- `--ensemble` - LightGBM + XGBoost ensemble
- `--cv-folds N` - –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–æ–ª–¥—ñ–≤ –¥–ª—è cross-validation

**evaluate_backtest.py:**
- `--history-dir` - –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –∑ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏
- `--initial-balance` - –ø–æ—á–∞—Ç–∫–æ–≤–∏–π –±–∞–ª–∞–Ω—Å
- `--risk-per-trade` - —Ä–∏–∑–∏–∫ –Ω–∞ —Ç—Ä–µ–π–¥ (% –≤—ñ–¥ –±–∞–ª–∞–Ω—Å—É)

---

## üìö –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –ø–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é

### –î–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ—ó —è–∫–æ—Å—Ç—ñ:
```bash
# 1. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
python scripts/make_dataset.py --config config.yaml --walk-forward

# 2. –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ensemble –º–æ–¥–µ–ª–µ–π
python scripts/train_lstm.py --config config.yaml --ensemble

# 3. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —ñ—Å—Ç–æ—Ä—ñ—ó
python historical_generator.py --config config.yaml --days 365

# 4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ meta-–¥–∞—Ç–∞—Å–µ—Ç—É
python label_generator.py

# 5. –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è meta-ensemble
python train_meta_model.py --ensemble --cv-folds 5

# 6. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–≥–Ω–∞–ª—ñ–≤
python scripts/infer_signals.py --config config.yaml
python infer_meta_signals.py

# 7. –û—Ü—ñ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
python evaluate_backtest.py
```

### –î–ª—è —à–≤–∏–¥–∫–æ–≥–æ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è:
```bash
# –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ GRU (—à–≤–∏–¥—à–∏–π –∑–∞ LSTM)
python scripts/train_lstm.py --config config.yaml --model-type gru

# –ë–µ–∑ augmentation (—à–≤–∏–¥—à–µ)
python scripts/train_lstm.py --config config.yaml --no-augment
```

---

## üéâ –í–∏—Å–Ω–æ–≤–æ–∫

–í—Å—ñ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤–ø—Ä–æ–≤–∞–¥–∂–µ–Ω—ñ —Ç–∞ –≥–æ—Ç–æ–≤—ñ –¥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è! –°–∏—Å—Ç–µ–º–∞ —Ç–µ–ø–µ—Ä –≤–∫–ª—é—á–∞—î:

‚úÖ 45+ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
‚úÖ 3 —Ç–∏–ø–∏ neural network –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä
‚úÖ Focal Loss + Class Weights
‚úÖ Data Augmentation
‚úÖ Walk-Forward Validation
‚úÖ LightGBM + XGBoost Ensemble
‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ñ —Ñ—ñ–Ω–∞–Ω—Å–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
‚úÖ Attention Mechanism
‚úÖ Time-Series Cross-Validation

**–û—á—ñ–∫—É–≤–∞–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —è–∫–æ—Å—Ç—ñ: 15-25%**

–î–ª—è –ø–∏—Ç–∞–Ω—å —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ –∑–≤–µ—Ä—Ç–∞–π—Ç–µ—Å—å –¥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó –∞–±–æ —Å—Ç–≤–æ—Ä—ñ—Ç—å issue –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä—ñ—ó.
